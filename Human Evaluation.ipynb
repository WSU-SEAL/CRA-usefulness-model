{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  precision_score\n",
    "from sklearn.metrics import  f1_score\n",
    "from statistics import mean\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN LABELED TEST SET\n",
    "input_file1 = 'data/samsung_cmp_test_human.csv'\n",
    "human_test = pd.read_csv(input_file1, error_bad_lines=False)\n",
    "\n",
    "input_file2 = 'data/samsung_cmp_test.csv'\n",
    "test = pd.read_csv(input_file2, error_bad_lines=False)\n",
    "\n",
    "base_df = pd.read_csv(\"data/new_data_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Labeler Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = (test[['label']].values.T[0]==3)*1\n",
    "Y_manual = (human_test[['label']].values.T[0]==3)*1\n",
    "pred = Y_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=precision_score(Y_test,pred, pos_label=1) #average=\"micro\")\n",
    "recall = recall_score(Y_test, pred, pos_label=1) #average=\"micro\")\n",
    "f1score = f1_score(Y_test, pred, pos_label=1) #average=\"micro\")\n",
    "minority_recall = recall_score(Y_test, pred, pos_label=0)\n",
    "minority_precision = precision_score(Y_test, pred, pos_label=0)\n",
    "minority_f1 = f1_score(Y_test, pred, pos_label=0)\n",
    "accuracy=accuracy_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Acc  | Useful                | Not Useful\n",
      "\t A    | P,      R,      F1    | P,      R,      F1\n",
      "Human:\t 61.88  |  86.81   61.72   72.15  |  28.99   62.5   39.6\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t Acc  | Useful                | Not Useful\")\n",
    "print(\"\\t A    | P,      R,      F1    | P,      R,      F1\")\n",
    "print(\"Human:\\t\", round(accuracy*100, 2), \" | \", round(precision*100, 2),\" \", round(recall*100, 2),\" \", round(f1score*100, 2), \" | \", round(minority_precision*100, 2),\" \", round(minority_recall*100, 2),\" \", round(minority_f1*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['author_file_experience',\n",
    " 'author_responded',\n",
    " 'code_word_ratio',\n",
    " 'dev_commits',\n",
    " 'gratitude',\n",
    " 'is_confirmatory_response',\n",
    " 'is_last_patch',\n",
    " 'line_change',\n",
    " 'message_sentiment',\n",
    " 'num_participate',\n",
    " 'num_patches',\n",
    " 'num_prev_comment_same_file',\n",
    " 'num_review_comment',\n",
    " 'patch_id',\n",
    " 'programming_words',\n",
    " 'question_ratio',\n",
    " 'readability',\n",
    " 'rev_commits',\n",
    " 'review_interval',\n",
    " 'similarity',\n",
    " 'status',\n",
    " 'stop_word_ratio',\n",
    " 'word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/new_data_processed_cmp_rc.csv') # WHOLE DATASET\n",
    "neat_data = pd.read_csv(\"data/neat_data_processed.csv\")\n",
    "neat_data['comment_id'] = df['comment_id']\n",
    "samsung_train_data = pd.read_csv('data/samsung_cmp_train_with_1484.csv') # DATA THAT WAS KEPT SEPARATE FOR TRAININGG\n",
    "train = pd.merge(neat_data, samsung_train_data['comment_id'], on='comment_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE DATA MINUS TRAIN DATA = TEST DATA\n",
    "test1 = neat_data[feature_names+['comment_id', 'label']]\n",
    "test2 = train[feature_names+['comment_id', 'label']]\n",
    "test = pd.concat([test1, test2])\n",
    "test = test.drop_duplicates(keep=False)[feature_names+['label']]\n",
    "train = train[feature_names+['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[feature_names]\n",
    "Y_train = train['label']\n",
    "X_test = test[feature_names]\n",
    "Y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Acc  | Useful                | Not Useful\n",
      "\t A    | P,      R,      F1    | P,      R,      F1\n",
      "Decision Tree &\t 77.94  &  88.82  &  82.85  &  85.71  &  46.32  &  58.28  &  51.47\n",
      "Random Forest &\t 80.89  &  88.42  &  87.62  &  87.93  &  54.2  &  53.98  &  53.32\n",
      "SVM &\t 73.43  &  86.67  &  78.57  &  80.9  &  45.45  &  52.86  &  45.93\n",
      "Neural Network &\t 74.75  &  86.15  &  81.36  &  82.45  &  45.66  &  48.32  &  44.33\n",
      "XGBoost &\t 76.28  &  86.48  &  83.2  &  83.79  &  47.91  &  48.59  &  46.06\n",
      "Logistic Regression &\t 75.46  &  86.44  &  82.06  &  83.33  &  45.79  &  49.04  &  45.34\n"
     ]
    }
   ],
   "source": [
    "#################[ CLASSIFIERS ]#############\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import  tree\n",
    "import  pydotplus\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  precision_score\n",
    "from sklearn.metrics import  f1_score\n",
    "from statistics import mean\n",
    "import random\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import  LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def normalize_df(df):\n",
    "    df_norm = (df - df.mean()) / (df.max() - df.min())\n",
    "    return df_norm\n",
    "\n",
    "def row_norm(df):\n",
    "    norm = df/np.sqrt(np.square(df).sum(axis=1))\n",
    "    return norm\n",
    "\n",
    "def standardize_df(df):\n",
    "    a = 1\n",
    "    df_norm = (df - df.mean(axis=a)) / df.std(axis=a)\n",
    "    return df_norm\n",
    "\n",
    "def benchmark_classifier(clf, X_train, Y_train, X_test, Y_test):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    precision=precision_score(Y_test,pred, pos_label=1) #average=\"micro\")\n",
    "    recall = recall_score(Y_test, pred, pos_label=1) #average=\"micro\")\n",
    "    f1score = f1_score(Y_test, pred, pos_label=1) #average=\"micro\")\n",
    "    minority_recall = recall_score(Y_test, pred, pos_label=0)\n",
    "    minority_precision = precision_score(Y_test, pred, pos_label=0)\n",
    "    minority_f1 = f1_score(Y_test, pred, pos_label=0)\n",
    "    accuracy=accuracy_score(Y_test,pred)\n",
    "\n",
    "    return (precision,recall,f1score,accuracy,minority_f1,minority_recall, minority_precision)\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=100 )\n",
    "svm = LinearSVC()\n",
    "nn  = MLPClassifier(alpha=1e-3, hidden_layer_sizes=(500,5), random_state=1)\n",
    "xgb = XGBClassifier()\n",
    "log = LogisticRegression()\n",
    "#lin = LinearRegression()\n",
    "\n",
    "clfs = [dt, rf, svm, nn, xgb, log]\n",
    "clf_names = ['Decision Tree', 'Random Forest', 'SVM', 'Neural Network', 'XGBoost', 'Logistic Regression']\n",
    "\n",
    "# def run_classifier(clf, iteration, resample, normalize):\n",
    "sampling_model = SMOTE(random_state=None, k_neighbors=15, sampling_strategy=0.65,  n_jobs=4)\n",
    "\n",
    "X_train_np=np.array(X_train)\n",
    "Y_train_np=np.array(Y_train)\n",
    "X_test_np=np.array(X_test)\n",
    "Y_test_np=np.array(Y_test)\n",
    "\n",
    "print(\"\\t Acc  | Useful                | Not Useful\")\n",
    "print(\"\\t A    | P,      R,      F1    | P,      R,      F1\")\n",
    "\n",
    "Precision=[]\n",
    "Recall=[]\n",
    "Fmean=[]\n",
    "Accuracy=[]\n",
    "Minority_recall = []\n",
    "Minority_precision = []\n",
    "Minority_F1 = []\n",
    "  \n",
    "for (clf, name) in zip(clfs, clf_names):\n",
    "    for i in range(20):\n",
    "        X_resampled, Y_resampled = sampling_model.fit_sample(X_train_np, Y_train_np)\n",
    "        (precision,recall,f1score,accuracy,minority_f1,\n",
    "         minority_recall,minority_precision)=benchmark_classifier(clf,X_resampled, Y_resampled,X_test_np,Y_test_np)\n",
    "        \n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        Fmean.append(f1score)\n",
    "        Accuracy.append(accuracy)\n",
    "        Minority_recall.append(minority_recall)\n",
    "        Minority_precision.append(minority_precision)\n",
    "        Minority_F1.append(minority_f1)\n",
    "        \n",
    "    (precision,recall,f1score,accuracy,minority_precision,minority_recall,minority_f1) = (\n",
    "        mean(Precision),\n",
    "        mean(Recall),\n",
    "        mean(Fmean),\n",
    "        mean(Accuracy),\n",
    "        mean(Minority_precision ),\n",
    "        mean(Minority_recall ),\n",
    "        mean(Minority_F1)\n",
    "    )\n",
    "    print(name,\"&\\t\", round(accuracy*100, 2), \" & \", round(precision*100, 2),\" & \", round(recall*100, 2),\" & \", \n",
    "          round(f1score*100, 2), \" & \", round(minority_precision*100, 2),\" & \", round(minority_recall*100, 2),\" & \", \n",
    "          round(minority_f1*100, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
